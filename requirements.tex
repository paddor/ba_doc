% vim: ft=tex
\chapter{Requirements}
The requirements gathered during the first meeting with the client are
explained in this chapter. These are more concrete than the ones listed in the
Task Description in \autoref{ch:task-desc}.

First of all, these are the priorities from the client's point of view in
descending order:

\begin{enumerate}
\item Clustering
\item Single-level \gls{HA}
\item Multi-level \gls{HA}
\item Persistence synchronization
\item Security (optional)
\item OPC UA \gls{HA} (optional)
\end{enumerate}

The following sections explain the requirements in greater detail.

\section{Functional}
This section elucidates the functional requirements, as opposed to the
non-functional requirements.

\subsection{Cluster}
This requirement is to allow running Roadster on multiple nodes in a
hierarchical topology.

Extending the \gls{CSP} to keep the \gls{DIM} in sync across all nodes is a central
part of the clustering functionality. This means replicating the
items within the DIM that are marked "dirty" (updated, but not synchronized yet) onto all other actors on all other nodes.

According to
the Data Model class diagram, these should only be instances of one of three
classes (marked yellow in the diagram):
\begin{itemize}
	\item \rb{DataItem}
	\item \rb{Session}
	\item \rb{Case}
\end{itemize}

In addition to that, there needs to be some kind of message routing so a user
of one node's web UI can send a command to another node where it will be
executed. An example for this is a forced value in the DIM to ignore the
actually measured value reported by a device in case the device is known to be
wrong.

It's important that every node subtree can live on autonomously even if the
link to its supernode or the supernode itself fails.

The above requirements imply that changes to the DIM can only be done by the
respective node. In other words, a node can only change its own values. It
cannot change values of supernodes, nor is it allowed to change values of
subnodes directly. This is to ensure that each node is its own source of truth
for all other nodes in the setup.


\subsubsection{Typical Usecases}
The typical use cases include:

\begin{description}
	\item [ Single level, single node ]
		This is the legacy setup and is what Roadster is already able to do.

	\item [ Single level \gls{HA} ]
		This is when there are exactly two nodes, both of them
		connected to the same PLC. There are two nodes for redundancy.

	\item [ Multi level, \gls{HA} at root only ]
		There can be multiple levels in the hierarchy, such as two or
		three (anything else is considered exotic), and there's a HA
		cluster at the root.
\end{description}

% TODO: illustrate using diagrams

The exotic cases which can be ignored, if need be, include:

\begin{itemize}
	\item Multi level, HA at bottom
	\item Multi level, HA in the middle
\end{itemize}


\subsection{Single level HA}
This is where each of the two nodes forming a HA unit is directly connected to a number of
subsystems such as \glspl{PLC}, forming two redundant network paths to each
subsystem. Both nodes are able to interact with the subsystems to perform
operation tasks (e.g. reading sensor data, writing down configurations), but
only one of them (the active one) must do so.

The two nodes must automatically find consensus on which one is currently active. The
passive one must automatically take over in case the active one is confirmed to
be dead. Certain memory ranges can be used by the nodes to help find consensus
without interfering with normal operation of the subsystem. The kinds of
failures that need to be handled include:
\begin{itemize}
	\item Hardware/software failure on the primary node
	\item Failure of one of the redundant networking paths connecting the subsystem to the two nodes
\end{itemize}


\subsection{Multi level HA}
This is where a node pair is the parent of one or more subnodes.

The types of failures that need to be handled include:
\begin{itemize}
	\item Software failure on the primary node, like an application or OS crash
	\item Hardware failure on the primary node, like a defect power supply
	\item Failure of the network link connecting a node to the cluster
\end{itemize}

All three failure types listed above can collectively be called \emph{crash},
as their effects are the same from the point of view of the cluster.

\subsection{Persistence synchronization}
This is about the synchronization of persisted data, which is currently stored
in TokyoCabinet databases on a Roadster node. With the clustering, this is
still true: Every node will have its own key-value store. Updates for persisted
data must only flow from south to north (towards the root node), so the root
node can collect and maintain a replication of the persisted data of all
subnodes, recursively.

It's important that every node and its subnodes form an autonomous subtree. So
in case the link to its supernode fails, it has to continue working. As soon as
the link is repaired, synchronization of the delta (the newly added data) can
be initiatd. Sending just the delta should be possible since keys in the
database contain timestamps.

This is not the same as \gls{CHP} (\gls{DIM} synchronization), as the DIM is shared across
all nodes and is a relatively small data structure. The TokyoCabinet databases
can possibly contain large amounts of data (in the hundreds of megabytes) and
are shared only towards the root node (thus "bubbling up").

100\% consistency is not an absolute requirement for persistence synchronization.
However, it is mandatory that updates make it to the root node within 30 seconds.

\subsection{OPC UA HA}
\emph{This is optional.}

A given \gls{HA} pair needs to provide an OPC UA Server Redundancy interface,
as described in \cite[6.4.2.4 Non-transparent Redundancy,
p.~96]{opc-ua:behavior:server-redundancy}.

% ---------------------------------------------------------------------------
\section{Use Cases}
The use cases are briefly described here. These should simply describe common
scenarios based on the requirements above. They can later be turned into
concrete testing scenarios.

\subsection{UC01: Cluster}
A root node R has two subnodes A and B. A user connected to the root node's web
UI wants to suppress a case repeatedly generated on subnode A. The DIM shall be
kept in sync across all nodes.

\subsection{UC02: Hardware failure at top}
A subnode A is connected to a root-level HA pair (nodes R1 and R2). The active
HA peer (R1) crashes. R2 is supposed to take over as soon as subnode A fails
over to R2.

\subsection{UC03: Hardware failure at bottom}
A HA pair is connected to a field device. The active nodes crashes. The passive
one has to notice and take over to ensure continued monitoring of and access to
the field device.

\subsection{UC04: Persistence synchronization}
A root node R has two subnodes A and B. Persisted data on A has to bubble up to
root, even after temporary failure of the link between them. Root eventually
contains the union of all subnodes' persisted data.

\subsection{UC05: OPC-UA HA}
A HA pair provides an OPC UA interface. On failover, the superordinate system
shall continue to interact with the leftover HA peer.

% ---------------------------------------------------------------------------
\section{Non-Functional Requirements}
The following subsections illustrate the non-functional requirements.

\subsection{Simplicity}
The two reoccuring patterns that surfaced during the requirements gathering
meeting were:

\begin{enumerate}
\item \gls{KISS} principle. Simplicity is favored, as experience shows that
	simpler systems are more stable, so complexity should be avoided if not
	absolutely necessary.

\item No premature optimization since it's the root of all evil.\footnote{Quote
	by Donald Knuth: ``Premature optimization is the root of all evil.''}
\end{enumerate}


\subsection{Testing}
Regarding testing, the following requirements exist:
\begin{itemize}
	\item the student's contributions are verified with at least unit tests
	\item use cases shall be integration tested in a close-to-reality setup
\end{itemize}

\subsection{Security}
\emph{This is optional. Also, this requirement has the lowest priority not
because it's insignificant, but because it's easy to enable transport level
security on ZMQ sockets.}

The communication between a given set of Roadster nodes must be secured using
encryption.
% TODO: wait for Andy's answer on #9
% TODO common concern about SCADA systems lacking security\\


\subsection{Coding Guidelines}
The coding guidelines desired by the client are basically the ones written down
in the popular Ruby style guide \cite{rb:style-guide}, with the following
differences or special remarks:

\begin{itemize}
	\item method calls: only use parenthesis when needed, even with arguments (as opposed to \footnote{\url{https://github.com/bbatsov/ruby-style-guide\#method-invocation-parens}})
	\item 2 blank lines before method definition (slightly extending \footnote{\url{https://github.com/bbatsov/ruby-style-guide\#empty-lines-between-methods}})
	\item YARD API doc, 1 blank comment line before param documentation, one blank comment line before code (ignoring \footnote{\url{https://github.com/bbatsov/ruby-style-guide\#rdoc-conventions}})
	\item Ruby 1.9 symbol keys are wanted (e.g. \rb{foo: "bar", baz: 42} instead of \rb{:foo => "bar", :baz => 42}, just like \footnote{\url{https://github.com/bbatsov/ruby-style-guide\#hash-literals}})
	\item align multiple assignments so there's a column of equal signs
\end{itemize}
